# Model arguments
model_name_or_path: /home/shared_models/huggingface/gemma-2-2b-it
torch_dtype: bfloat16
attn_implementation: eager

# LoRA arguments
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: all-linear

# Data training arguments
chat_template: "{% if messages|length>1 %}{{ bos_token }}{% endif %}{% if messages[0]['role']=='system' %}{% set system_message=messages[0]['content']|trim+'\n\n' %}{% set messages=messages[1:] %}{% else %}{% set system_message='' %}{% endif %}{% for message in messages %}{% if loop.index0==0 %}{% set content=system_message+message['content'] %}{% else %}{% set content=message['content'] %}{% endif %}{% if message['role']=='assistant' %}{% set role='model' %}{% else %}{% set role=message['role'] %}{% endif %}{{ '<start_of_turn>'+role+'\n'+content|trim+'<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<start_of_turn>model\n' }}{% endif %}"
dataset_mixer:
  jonluj/lowercase_ultrachat: 0.3
dataset_splits:
- train_sft
- test_sft
preprocessing_num_workers: 12

# SFT trainer config
bf16: true
dataset_text_field: text
do_eval: true
eval_strategy: steps
eval_steps: 20
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 2.0e-04
log_level: info
logging_steps: 5
logging_strategy: steps
lr_scheduler_type: cosine
max_seq_length: 2048
num_train_epochs: 1
output_dir: outputs/gemma2-2b-lowercase-sft-lora
overwrite_output_dir: true
per_device_eval_batch_size: 16
per_device_train_batch_size: 8
push_to_hub: false
report_to: wandb
resume_from_checkpoint: false
run_name: gemma2-2b-lowercase-sft-lora
save_strategy: "steps"
save_steps: 20
seed: 42
warmup_ratio: 0.1