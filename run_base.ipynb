{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "# base_path = \"/home/shared_models/huggingface/gemma-2-2b-it/\"\n",
    "base_path = \"gemma2-2b-lowercase-sft\"\n",
    "tokenizer_path = \"/home/shared_models/huggingface/gemma-2-2b-it/\"\n",
    "\n",
    "use_quantization = True\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_storage=\"uint8\",\n",
    "    )\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config if use_quantization else None,\n",
    "    use_cache=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "gen_config = GenerationConfig(max_new_tokens=120, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story about a time traveler changing the course of world war II\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell me a story about a time traveler changing the course of world war II<end_of_turn>\n",
      "<start_of_turn>model\n",
      "the year is 1942. the world is at war, and the battle for europe is raging. but in a secret laboratory deep beneath the ground, a time traveler named elias is working on a project that could change the course of the war. elias has been sent back in time to observe and learn from the past, but he has also been tasked with altering events to prevent the worst possible outcomes.\n",
      "\n",
      "elias's first mission is to prevent the assassination of hitler. he knows that hitler's death would have a ripple effect on the war, and he\n"
     ]
    }
   ],
   "source": [
    "outputs = base_model.generate(inputs, generation_config=gen_config)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
